# app/crawling/dbsetup_pipeline.py
# ëª©ì : district, welfare, ehealth í¬ë¡¤ëŸ¬ â†’ DB ì—…ë¡œë“œ â†’ policy_id ê·¸ë£¨í•‘
# ì¤‘ê°„ JSON ì—†ì´, ë©”ëª¨ë¦¬ì—ì„œ ë°”ë¡œ documents/embeddingsì— ì‚½ì…
# ì§„í–‰ë¥ (%) ì¶œë ¥ ì¶”ê°€ ë²„ì „

import os, sys, argparse, traceback, json
from datetime import datetime
import psycopg2
from psycopg2.extras import execute_values
from openai import OpenAI
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë³´ì •
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.insert(0, PROJECT_ROOT)

from app.crawling.crawlers.district_crawler import HealthCareWorkflow
from app.crawling.crawlers.welfare_crawler import WelfareCrawler
from app.crawling.crawlers.ehealth_crawler import EHealthCrawler
from app.crawling.crawlers import run_all_crawlers as rac
from app.dao.db_policy import dbuploader_policy as dbuploader
from app.dao.db_policy import dbgrouper_policy as dbgrouper
from app.dao.utils_db import eprint, extract_sitename_from_url, get_weight


def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)
    return p


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìˆ˜ì§‘ í•¨ìˆ˜ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_district(urls, out_dir):
    all_data = []
    for url in urls:
        wf = HealthCareWorkflow(output_dir=out_dir)
        summary = wf.run(start_url=url, save_links=True, save_json=False, return_data=True)
        all_data.extend(summary.get("data", []))
    return all_data


def collect_welfare(out_dir, no_filter=False, max_items=None):
    crawler = WelfareCrawler(output_dir=out_dir)
    data = crawler.run_workflow(filter_health=not no_filter, max_items=max_items,
                                return_data=True, save_json=False)
    return data or []


def collect_ehealth(out_dir, categories=None, max_pages=None):
    crawler = EHealthCrawler(output_dir=out_dir)
    data = crawler.run_workflow(categories=categories, max_pages_per_category=max_pages,
                                return_data=True, save_json=False)
    return data or []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DB ì—…ë¡œë“œ (ì§„í–‰ë¥  % í‘œì‹œ ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def upload_records(records, reset="none", emb_model="text-embedding-3-small", commit_every=50):
    if not records:
        eprint("[upload] ì—…ë¡œë“œí•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    preprocess_title = dbuploader.preprocess_title
    get_embedding = dbuploader.get_embedding

    load_dotenv()
    DB_URL = os.getenv("DATABASE_URL")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    if not DB_URL or not OPENAI_API_KEY:
        raise RuntimeError("DATABASE_URL, OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    _ = OpenAI(api_key=OPENAI_API_KEY)

    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()

    total = len(records)
    bar_length = 40  # ì§„í–‰ë¥  ë°” ê¸¸ì´

    try:
        if reset != "none":
            cur.execute("TRUNCATE TABLE embeddings, documents RESTART IDENTITY CASCADE;")
            conn.commit()
            print(f"âœ… í…Œì´ë¸” ë¦¬ì…‹ ì™„ë£Œ: {reset}")

        inserted = 0
        for idx, item in enumerate(records, 1):
            title = item.get("title", "")
            requirements = item.get("support_target", "")
            benefits = item.get("support_content", "")
            raw_text = item.get("raw_text", "")
            url = item.get("source_url", "")
            region = item.get("region", "")
            sitename = extract_sitename_from_url(url)
            weight = get_weight(region, sitename)

            # documents
            cur.execute("""
                INSERT INTO documents (title, requirements, benefits, raw_text, url, policy_id, region, sitename, weight, llm_reinforced, llm_reinforced_sources)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING id;
            """, (title, requirements, benefits, raw_text, url, None, region, sitename, weight, False, None))
            doc_id = cur.fetchone()[0]

            # embeddings
            emb_rows = []
            title_emb_text = preprocess_title(title)
            for fname, text_value in (
                ("title", title_emb_text),
                ("requirements", requirements),
                ("benefits", benefits),
            ):
                vec = get_embedding(text_value, emb_model)
                if vec:
                    emb_rows.append((doc_id, fname, vec))

            if emb_rows:
                execute_values(
                    cur,
                    "INSERT INTO embeddings (doc_id, field, embedding) VALUES %s",
                    emb_rows,
                    template="(%s, %s, %s)"
                )

            inserted += 1
            percent = (inserted / total) * 100
            filled = int(bar_length * percent / 100)
            bar = "â–ˆ" * filled + "-" * (bar_length - filled)
            sys.stdout.write(f"\r[Upload] |{bar}| {percent:6.2f}% ({inserted}/{total})")
            sys.stdout.flush()

            if inserted % commit_every == 0:
                conn.commit()

        conn.commit()
        print(f"\nğŸ‰ ì—…ë¡œë“œ ì™„ë£Œ! ì´ {inserted}ê±´ ì‚½ì…")

    except Exception as e:
        conn.rollback()
        eprint(f"[upload] ì—ëŸ¬ë¡œ ë¡¤ë°±: {e}")
        raise
    finally:
        cur.close()
        conn.close()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê·¸ë£¹í•‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def group_policies(threshold=0.85, batch_size=500, reset_all=False, verbose=True):
    res = dbgrouper.assign_policy_ids(
        title_field="title",
        similarity_threshold=threshold,
        batch_size=batch_size,
        dry_run=False,
        reset_all_on_start=reset_all,
        verbose=verbose,
    )
    return res


def _get_runall_urls():
    for name in ["TARGET_URLS", "DISTRICT_TARGETS", "DEFAULT_URLS", "URLS"]:
        if hasattr(rac, name):
            v = getattr(rac, name)
            try:
                return list(v)
            except Exception:
                pass
    return []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    p = argparse.ArgumentParser(description="í†µí•© í¬ë¡¤ë§ â†’ DB ì—…ë¡œë“œ â†’ policy_id ê·¸ë£¨í•‘ (in-memory, ì§„í–‰ë¥  í‘œì‹œ)")
    p.add_argument("--source", choices=["district", "welfare", "ehealth", "all"], default="district")
    p.add_argument("--urls", nargs="*", help="district ì‹œì‘ URLë“¤")
    p.add_argument("--out-dir", default=os.path.join(PROJECT_ROOT, "app", "crawling", "output"))
    p.add_argument("--reset", choices=["none", "truncate"], default="none")
    p.add_argument("--group", action="store_true")
    p.add_argument("--threshold", type=float, default=0.85)
    p.add_argument("--batch-size", type=int, default=500)
    p.add_argument("--use-runall-targets", action="store_true", help="district ìˆ˜ì§‘ ì‹œ run_all_crawlers.pyì˜ URL ëª©ë¡ ì‚¬ìš©")
    args = p.parse_args()

    _ensure_dir(args.out_dir)

    try:
        mem_data = []

        if args.source in ("district", "all"):
            if args.use_runall_targets:
                urls = _get_runall_urls()
                if not urls:
                    eprint("[district] run_all_crawlers.pyì—ì„œ URLì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. --urls ì¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                    urls = args.urls or []
            else:
                urls = args.urls or [
                    "https://health.gangnam.go.kr/web/business/support/sub01.do",
                    "https://health.gangdong.go.kr/health/site/main/content/GD20030100",
                    "https://www.gangbuk.go.kr/health/main/contents.do?menuNo=400151",
                    "https://www.gangseo.seoul.kr/health/ht020231",
                    "https://www.gwanak.go.kr/site/health/05/10502010600002024101710.jsp",
                    "https://www.gwangjin.go.kr/health/main/contents.do?menuNo=300080",
                    "https://www.guro.go.kr/health/contents.do?key=1320&",
                    "https://www.dongjak.go.kr/healthcare/main/contents.do?menuNo=300342",
                    "https://www.sdm.go.kr/health/contents/infectious/law",
                    "https://www.seocho.go.kr/site/sh/03/10301000000002015070902.jsp",
                    "https://www.sb.go.kr/bogunso/contents.do?key=6553",
                    "https://www.ydp.go.kr/health/contents.do?key=6073&",
                    "https://www.songpa.go.kr/ehealth/contents.do?key=4525&",
                    "https://jongno.go.kr/Health.do?menuId=401309&menuNo=401309",
                ]

            eprint(f"[district] {len(urls)}ê°œ URL ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ìˆ˜ì§‘)")
            mem_data += collect_district(urls, args.out_dir)

        if args.source in ("welfare", "all"):
            mem_data += collect_welfare(args.out_dir)

        if args.source in ("ehealth", "all"):
            mem_data += collect_ehealth(args.out_dir)

        if not mem_data:
            eprint("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        eprint(f"[upload] ë©”ëª¨ë¦¬ ë ˆì½”ë“œ {len(mem_data)}ê±´ ì—…ë¡œë“œ ì¤‘â€¦")
        upload_records(mem_data, reset=args.reset)

        if args.group:
            eprint("[group] policy_id ê·¸ë£¨í•‘ ì‹œì‘")
            result = group_policies(args.threshold, args.batch_size)
            print("[group result]", result)

        print("\nâœ… ì™„ë£Œ:", len(mem_data), "records")

    except Exception as e:
        traceback.print_exc()
        eprint(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
